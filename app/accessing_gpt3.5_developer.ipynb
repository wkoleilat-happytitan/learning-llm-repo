{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AhnkvywOmCd6JcObtH0lY7ugsWTfk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain is a blockchain platform that focuses on providing language-related services such as translation, interpretation, and language learning. It aims to create a decentralized ecosystem for language professionals and users to interact and exchange services.\\n\\nOn the other hand, LlamaIndex is a decentralized finance (DeFi) platform that focuses on providing users with a range of financial services, such as lending, borrowing, and asset management. It aims to provide users with easy access to decentralized financial products and services.\\n\\nIn summary, LangChain is focused on language-related services while LlamaIndex is focused on decentralized finance services.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735002729, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=118, prompt_tokens=19, total_tokens=137, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain and LlamaIndex are two entirely different platforms serving different purposes in the cryptocurrency and blockchain space.\n",
       "\n",
       "LangChain is a decentralized finance platform built on the Ethereum blockchain that aims to provide users with access to a range of financial services, including lending, borrowing, and trading. The platform uses smart contracts to automate these services and reduce the need for intermediaries. LangChain also offers its native token, LANG, which can be used for various purposes within the platform.\n",
       "\n",
       "On the other hand, LlamaIndex is a data analytics platform that aggregates and analyses various data points from the cryptocurrency market to provide users with insights and information on different cryptocurrencies and tokens. It allows users to track market trends, trading volumes, historical price data, and other metrics to make informed decisions when investing or trading in cryptocurrencies. LlamaIndex does not offer financial services like lending or borrowing but focuses solely on data analysis and reporting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I really don't have a preference because I'm not capable of eating ice. But that's beside the point. I'm angry because I'm extremely hungry and you keep asking me about ice! Do you have any food to offer or are you just here to taunt me with questions about frozen water?!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm having a great day, thank you for asking! As for your question, I don't have a preference for crushed or cubed ice since I don't eat or drink. But if I had to choose, crushed ice can be more versatile in certain drinks, while cubed ice is more traditional. What about you, do you have a preference?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Ahnm3qRsXcx95fJVIosCvXkPOSVyG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm having a great day, thank you for asking! As for your question, I don't have a preference for crushed or cubed ice since I don't eat or drink. But if I had to choose, crushed ice can be more versatile in certain drinks, while cubed ice is more traditional. What about you, do you have a preference?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1735002799, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=73, prompt_tokens=30, total_tokens=103, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The stimple ingredient in the soup was the secret falbean spice blend that gave it a unique and flavorful taste."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\"I couldn't believe how quickly and efficiently the stimple drill and falbean wrench worked together to assemble the furniture.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, it does matter which travel option Billy selects. \n",
       "\n",
       "If Billy flies and then takes a bus, it will take a total of 5 hours (3hrs flight + 2hrs bus), which means he will arrive at his destination at 6PM local time.\n",
       "\n",
       "If Billy takes the teleporter and then a bus, it will take a total of 1 hour (0hrs teleporter + 1hrs bus), meaning he will arrive at his destination at 2PM local time. \n",
       "\n",
       "Therefore, Billy should choose the teleporter option in order to get home before 7PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, it does matter which travel option Billy selects.\n",
       "\n",
       "If Billy flies and then takes a bus, it will take him a total of 5 hours to get home. Since it's currently 1PM local time, he will arrive at 6PM local time, which is before 7PM EDT. Therefore, this option will ensure Billy gets home before 7PM EDT.\n",
       "\n",
       "If Billy takes the teleporter and then a bus, it will take him a total of 1 hour to get home. However, since it's currently 1PM local time, he will arrive at 2PM local time, which is before 7PM EDT. Therefore, this option also ensures Billy gets home before 7PM EDT.\n",
       "\n",
       "In conclusion, both travel options will allow Billy to get home before 7PM EDT. However, the teleporter option is a quicker and more efficient choice, as it only takes 1 hour compared to the 5 hours it would take if Billy flew and then took a bus."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
